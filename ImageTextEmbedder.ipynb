{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "oVPUAUKlU6JV",
      "metadata": {
        "id": "oVPUAUKlU6JV"
      },
      "outputs": [],
      "source": [
        "!pip install open-clip-torch faiss-cpu --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "M1IKEKtMViwX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1IKEKtMViwX",
        "outputId": "1fadfeb2-ae36-4c96-85aa-9064b05b5ce3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "AWH1tHUTYgkz",
      "metadata": {
        "id": "AWH1tHUTYgkz"
      },
      "outputs": [],
      "source": [
        "!pip install openpyxl --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6AEloliUVm-r",
      "metadata": {
        "id": "6AEloliUVm-r"
      },
      "outputs": [],
      "source": [
        "#STEP 3: Set up paths and import libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import open_clip\n",
        "import faiss\n",
        "import pickle\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "duU2L13AWgvf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duU2L13AWgvf",
        "outputId": "b0c43d86-6a72-4da3-a316-79d3cead712c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/open_clip/factory.py:388: UserWarning: These pretrained weights were trained with QuickGELU activation but the model config does not have that enabled. Consider using a model config with a \"-quickgelu\" suffix or enable with a flag.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, _, preprocess = open_clip.create_model_and_transforms(\"ViT-B-32\", pretrained=\"openai\")\n",
        "model = model.to(device)\n",
        "tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ZkOkiTL_W9TT",
      "metadata": {
        "id": "ZkOkiTL_W9TT"
      },
      "outputs": [],
      "source": [
        "products_df = pd.read_excel(\"/content/drive/MyDrive/Colab Notebooks/product_data.xlsx\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "GvelkRULcKU4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvelkRULcKU4",
        "outputId": "f8e5a1c6-3b23-428e-95cf-3a16013722ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "replace /content/drive/MyDrive/Colab Notebooks/images/images/116103_11358.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: no\n",
            "replace /content/drive/MyDrive/Colab Notebooks/images/images/116103_11359.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: none\n",
            "replace /content/drive/MyDrive/Colab Notebooks/images/images/116103_11360.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!unzip -q \"/content/drive/MyDrive/Colab Notebooks/images.zip\" -d \"/content/drive/MyDrive/Colab Notebooks/images\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "n-9TzbOLVmfq",
      "metadata": {
        "id": "n-9TzbOLVmfq"
      },
      "outputs": [],
      "source": [
        "image_dir = \"/content/drive/MyDrive/Colab Notebooks/images/images\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "n8UF0Oy9o_Bc",
      "metadata": {
        "id": "n8UF0Oy9o_Bc"
      },
      "outputs": [],
      "source": [
        "def row_to_text(row):\n",
        "    parts = []\n",
        "    for col in row.index:\n",
        "        value = row[col]\n",
        "        if pd.notnull(value):\n",
        "            parts.append(f\"{col}: {value}\")\n",
        "    return \" \".join(parts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Dz1KTYi0ZTTG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dz1KTYi0ZTTG",
        "outputId": "cab1a136-43fa-4c89-8919-baccd4e349de"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 969/969 [02:23<00:00,  6.77it/s]\n"
          ]
        }
      ],
      "source": [
        "# # Map base IDs to image filenames\n",
        "available_images = {}\n",
        "for f in os.listdir(image_dir):\n",
        "    if f.endswith(\".jpg\"):\n",
        "        base_id = f.split(\"_\")[0]  # Extract the ID before underscore\n",
        "        if base_id not in available_images:\n",
        "            available_images[base_id] = f  # Only keep the first match\n",
        "\n",
        "# # Embed\n",
        "# embeddings = []\n",
        "# meta_data = []\n",
        "\n",
        "# print(\"generating embeddings from local images...\")\n",
        "\n",
        "# for idx, row in tqdm(products_df.iterrows(), total=len(products_df)):\n",
        "#     product_id = str(row[\"id\"])\n",
        "\n",
        "#     matching_file = available_images.get(product_id)\n",
        "#     if not matching_file:\n",
        "#         continue\n",
        "\n",
        "#     try:\n",
        "#         image_path = os.path.join(image_dir, matching_file)\n",
        "#         image = Image.open(image_path).convert(\"RGB\")\n",
        "#         image_input = preprocess(image).unsqueeze(0).to(device)\n",
        "\n",
        "#         text = f\"{row['title']} {row.get('category', '')}\"\n",
        "#         text_input = tokenizer(text).unsqueeze(0).to(device)\n",
        "\n",
        "#         with torch.no_grad():\n",
        "#             image_feat = model.encode_image(image_input)\n",
        "#             text_feat = model.encode_text(text_input)\n",
        "#             combined = (image_feat + text_feat) / 2\n",
        "#             combined = combined / combined.norm(dim=-1, keepdim=True)\n",
        "\n",
        "#         embeddings.append(combined.cpu().squeeze().numpy())\n",
        "#         meta_data.append(row.to_dict())\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"failed to process ID {row['id']}: {e}\")\n",
        "# Embed only products that have a corresponding image\n",
        "embeddings = []\n",
        "meta_data = []\n",
        "\n",
        "for idx, row in tqdm(products_df.iterrows(), total=len(products_df)):\n",
        "    try:\n",
        "        product_id = str(row[\"id\"])\n",
        "        matching_file = available_images.get(product_id)\n",
        "\n",
        "        if not matching_file:\n",
        "            continue  # to skip if image is not available\n",
        "\n",
        "        # Combine all row fields into a single text\n",
        "        text = row_to_text(row)\n",
        "\n",
        "        # Load image\n",
        "        image_path = os.path.join(image_dir, matching_file)\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        image_input = preprocess(image).unsqueeze(0).to(device)  # [1, 3, H, W]\n",
        "\n",
        "        # Tokenize text (DO NOT UNSQUEEZE)\n",
        "        text_input = tokenizer(text).to(device)  # already [1, seq_len]\n",
        "\n",
        "        # Encode both\n",
        "        with torch.no_grad():\n",
        "            image_feat = model.encode_image(image_input)\n",
        "            text_feat = model.encode_text(text_input)\n",
        "\n",
        "            combined = (image_feat + text_feat) / 2\n",
        "            combined = combined / combined.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        embeddings.append(combined.cpu().squeeze().numpy())\n",
        "        meta_data.append(row.to_dict())\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to process ID {row['id']}: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "VxP1TodJZvaD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxP1TodJZvaD",
        "outputId": "dcb13f13-c495-4e9f-c6be-ed3759198617"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total successful embeddings: 967\n"
          ]
        }
      ],
      "source": [
        "print(f\"Total successful embeddings: {len(embeddings)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d50d415a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "d50d415a",
        "outputId": "e30012de-8b0d-42ba-b55d-9662a767f069",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "# Save FAISS index\n",
        "embedding_matrix = np.vstack(embeddings).astype(\"float32\")\n",
        "index = faiss.IndexFlatIP(embedding_matrix.shape[1])\n",
        "os.makedirs(\"utils\", exist_ok=True)\n",
        "faiss.write_index(index, \"utils/faiss_catalog.index\")\n",
        "with open(\"utils/catalog_meta.pkl\", \"wb\") as f:\n",
        "    pickle.dump(meta_data, f)\n",
        "\n",
        "print(f\"\\nEmbedded and saved FAISS index for {len(meta_data)} products.\")\n",
        "\n",
        "# to download the faiss\n",
        "import shutil\n",
        "shutil.make_archive(\"faiss_output\", 'zip', \"utils\")\n",
        "from google.colab import files\n",
        "files.download(\"faiss_output.zip\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
